{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ./data_model/...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to ./data_model/...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#NLP libraries\n",
    "import nltk\n",
    "from gensim.models import Doc2Vec\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim import corpora\n",
    "import textstat\n",
    "\n",
    "#Machine learning libraries\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Helper libraries\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "np.random.seed(2023)\n",
    "\n",
    "\n",
    "nltk.download('punkt', download_dir=\"./data_model/\")\n",
    "nltk.download(\"stopwords\", download_dir=\"./data_model/\")\n",
    "nltk.data.path.append(os.path.abspath(\"./data_model/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "print(_stopwords)\n",
    "\n",
    "def clean(text):  #Removing unecessary punctuation and all lower case.\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
    "    text = text.replace('„','')\n",
    "    text = text.replace('“','')\n",
    "    text = text.replace('\"','')\n",
    "    text = text.replace('\\'','')\n",
    "    text = text.replace('-','')\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(content):\n",
    "    for word in _stopwords:\n",
    "        content = content.replace(' '+word+' ',' ')\n",
    "    return content\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 3:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>second crusade  2007 schools wikipedia selecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>navassa island  2007 schools wikipedia selecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>evan rachel wood  2007 schools wikipedia selec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tropical storm henri (2003)  2007 schools wiki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>final fantasy adventure  2007 schools wikipedi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>réunion  2007 schools wikipedia selection. rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>flower  2007 schools wikipedia selection. rela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4601</th>\n",
       "      <td>banknote  2007 schools wikipedia selection. re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4602</th>\n",
       "      <td>weyto language  2007 schools wikipedia selecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4603</th>\n",
       "      <td>marseille  2007 schools wikipedia selection. r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4604 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           text_content\n",
       "0     second crusade  2007 schools wikipedia selecti...\n",
       "1     navassa island  2007 schools wikipedia selecti...\n",
       "2     evan rachel wood  2007 schools wikipedia selec...\n",
       "3     tropical storm henri (2003)  2007 schools wiki...\n",
       "4     final fantasy adventure  2007 schools wikipedi...\n",
       "...                                                 ...\n",
       "4599  réunion  2007 schools wikipedia selection. rel...\n",
       "4600  flower  2007 schools wikipedia selection. rela...\n",
       "4601  banknote  2007 schools wikipedia selection. re...\n",
       "4602  weyto language  2007 schools wikipedia selecti...\n",
       "4603  marseille  2007 schools wikipedia selection. r...\n",
       "\n",
       "[4604 rows x 1 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file_dir=\"./data/wikispeedia_articles_plaintext/plaintext_articles/\"\n",
    "file_data=[]\n",
    "\n",
    "for filename in os.listdir(text_file_dir):\n",
    "    with open(os.path.join(text_file_dir, filename), \"r\") as file:\n",
    "        content = file.read()\n",
    "    \n",
    "        # Split the content into lines to remove the header\n",
    "    lines = content.split('\\n')\n",
    "\n",
    "    #Removing the header (line 0)\n",
    "    if lines:\n",
    "        lines.pop(0)\n",
    "\n",
    "            \n",
    "\n",
    "    # Making it an array and removing all \\n\n",
    "    content = '\\n'.join(lines)\n",
    "    content = content.replace(\"\\n\", \" \")\n",
    "\n",
    "\n",
    "    new_file_content= {\"text_content\": content}\n",
    "    \n",
    "    file_data.append(new_file_content)\n",
    "\n",
    "project_data = pd.DataFrame(file_data)\n",
    "\n",
    "project_data['text_content'] = project_data['text_content'].apply(clean)\n",
    "project_data['text_content'] = project_data['text_content'].apply(remove_stopwords)\n",
    "\n",
    "project_tagged = project_data.apply(\n",
    "   lambda r: TaggedDocument(words=tokenize_text(r['text_content']), tags=  []), axis=1)\n",
    "\n",
    "project_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.025*\"national\" + 0.012*\"park\" + 0.007*\"historic\" + 0.006*\"monument\" + 0.005*\"site\" + 0.005*\"memorial\" + 0.005*\"area\" + 0.004*\"new\" + 0.003*\"affiliated\" + 0.003*\"virginia\"\n",
      "Topic: 1 \n",
      "Words: 0.059*\"national\" + 0.019*\"park\" + 0.012*\"site\" + 0.010*\"historic\" + 0.009*\"area\" + 0.009*\"monument\" + 0.008*\"new\" + 0.008*\"historical\" + 0.007*\"washington\" + 0.006*\"memorial\"\n",
      "Topic: 2 \n",
      "Words: 0.014*\"national\" + 0.006*\"park\" + 0.006*\"historic\" + 0.005*\"site\" + 0.004*\"monument\" + 0.003*\"area\" + 0.003*\"memorial\" + 0.003*\"new\" + 0.002*\"washington\" + 0.002*\"historical\"\n",
      "Topic: 3 \n",
      "Words: 0.015*\"national\" + 0.004*\"park\" + 0.004*\"monument\" + 0.004*\"site\" + 0.004*\"historic\" + 0.003*\"area\" + 0.003*\"memorial\" + 0.003*\"new\" + 0.002*\"historical\" + 0.002*\"affiliated\"\n",
      "Topic: 4 \n",
      "Words: 0.021*\"national\" + 0.008*\"park\" + 0.007*\"historic\" + 0.005*\"monument\" + 0.005*\"site\" + 0.004*\"new\" + 0.004*\"memorial\" + 0.004*\"area\" + 0.003*\"washington\" + 0.003*\"recreation\"\n",
      "Topic: 5 \n",
      "Words: 0.100*\"national\" + 0.029*\"park\" + 0.021*\"historic\" + 0.015*\"site\" + 0.015*\"monument\" + 0.014*\"area\" + 0.011*\"new\" + 0.010*\"memorial\" + 0.009*\"historical\" + 0.007*\"washington\"\n",
      "Topic: 6 \n",
      "Words: 0.138*\"national\" + 0.043*\"park\" + 0.030*\"historic\" + 0.025*\"site\" + 0.023*\"monument\" + 0.018*\"area\" + 0.015*\"memorial\" + 0.014*\"new\" + 0.013*\"washington\" + 0.011*\"historical\"\n",
      "Topic: 7 \n",
      "Words: 0.059*\"national\" + 0.023*\"park\" + 0.016*\"historic\" + 0.013*\"site\" + 0.012*\"monument\" + 0.010*\"memorial\" + 0.010*\"new\" + 0.007*\"historical\" + 0.007*\"area\" + 0.006*\"washington\"\n",
      "Topic: 8 \n",
      "Words: 0.071*\"national\" + 0.024*\"park\" + 0.016*\"historic\" + 0.013*\"site\" + 0.012*\"monument\" + 0.010*\"memorial\" + 0.009*\"area\" + 0.009*\"new\" + 0.007*\"affiliated\" + 0.007*\"historical\"\n",
      "Topic: 9 \n",
      "Words: 0.089*\"national\" + 0.028*\"park\" + 0.025*\"historic\" + 0.024*\"monument\" + 0.020*\"site\" + 0.012*\"new\" + 0.011*\"memorial\" + 0.011*\"area\" + 0.010*\"historical\" + 0.009*\"washington\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "dictionary = corpora.Dictionary(project_tagged [567])\n",
    "\n",
    "# Create a document-term matrix\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in project_tagged[567]]\n",
    "\n",
    "# Create an LDA model using gensim\n",
    "lda_model = gensim.models.LdaMulticore(doc_term_matrix, num_topics=10, id2word=dictionary, passes=2, workers=2)\n",
    "\n",
    "# Print topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         title  \\\n",
      "0               Second Crusade   \n",
      "1               Navassa Island   \n",
      "2             Evan Rachel Wood   \n",
      "3  Tropical Storm Henri (2003)   \n",
      "4      Final Fantasy Adventure   \n",
      "\n",
      "                                        text_content  \n",
      "0     #copyright   2007 Schools Wikipedia Selecti...  \n",
      "1     #copyright   2007 Schools Wikipedia Selecti...  \n",
      "2     #copyright   2007 Schools Wikipedia Selecti...  \n",
      "3     #copyright   2007 Schools Wikipedia Selecti...  \n",
      "4     #copyright   2007 Schools Wikipedia Selecti...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "text_file_dir = \"./data/wikispeedia_articles_plaintext/plaintext_articles/\"\n",
    "file_data = []\n",
    "\n",
    "for filename in os.listdir(text_file_dir):\n",
    "    if filename.endswith(\".txt\"):  # Ensure you're only reading .txt files\n",
    "        with open(os.path.join(text_file_dir, filename), 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "            # Split the content into lines to remove the header\n",
    "            lines = content.split('\\n')\n",
    "\n",
    "            # Removing the header (line 0) and join back the content\n",
    "            if lines:\n",
    "                title = lines.pop(2)  # Assuming the first line is the title\n",
    "            \n",
    "            content = ' '.join(lines).replace(\"\\n\", \" \")\n",
    "            \n",
    "            # Create a dictionary with title and content\n",
    "            file_data.append({\"title\": title, \"text_content\": content})\n",
    "\n",
    "# Create a DataFrame\n",
    "project_data = pd.DataFrame(file_data)\n",
    "\n",
    "# Display the DataFrame structure\n",
    "print(project_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Flesch Reading-Ease score for the text is: 54.73\n",
      "The Gunning Fog Index for the text is: 10.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Navassa Island  2007 Schools Wikipedia Selection. Related subjects: North American Geography     Navassa Island map from The World Factbook    Enlarge    Navassa Island map from The World Factbook     Navassa Island (French: La Navase, Haitian Kreyòl: Lanavaz or Lavash)    is a small, uninhabited island in the Caribbean Sea. The government of    the United States claims the island as an unorganized unincorporated    territory, part of the United States Minor Outlying Islands, where it    is administered by the U.S. Fish and Wildlife Service. However, the    island is also claimed by Haiti.  Geography and Topography     Navassa Island is about two square miles (5.2 km²). It is found at a    strategic location 160 km (90 nautical miles) south of the U.S. naval    base at Guantanamo Bay, Cuba, about one-fourth of the way from Haiti to    Jamaica in the Jamaica Channel. It reaches an elevation of 77 m at an    unnamed peak 100 m south of the lighthouse, Navassa Island Light. This    location is 400 m from the southwestern coast or 600 m east of Lulu    Bay. The island\\'s latitude and longitude is 18°24′0″N, 75°0′30″W.    Navassa Island is south of Cuba, east of Jamaica, and west of Haiti.    This map originates with the US government and shows the US claim on    the island    Enlarge    Navassa Island is south of Cuba, east of Jamaica, and west of Haiti.    This map originates with the US government and shows the US claim on    the island    Navassa Island - NASA NLT Landsat 7 (visible color) satellite image    Enlarge    Navassa Island - NASA NLT Landsat 7 (visible colour) satellite image     The terrain of Navassa Island consists mostly of exposed coral and    limestone, the island being ringed by vertical white cliffs nine to 15    meters high, but with enough grassland to support goat herds. There are    also dense stands of fig-like trees and scattered cactus on the island.    Its topography and ecology is similar to that of Mona Island, a small    limestone island located in the Mona Channel, between Puerto Rico and    the Dominican Republic. Historically, it shares the same similarities    as Navassa Island since both are U.S. territories, were once centers of    guano mining, and presently are nature reserves. Transient Haitian    fishermen and others camp on the island but the island is otherwise    uninhabited. It has no ports or harbors, only offshore anchorages, and    its only natural resource is guano; economic activity consists of    subsistence fishing and commercial trawling activities.  History     In 1504, Christopher Columbus, stranded on Jamaica, sent some crew    members by canoe to Hispaniola for help. They ran into the island on    the way, but it had no water. They called it Navaza (from \"nava-\"    meaning plain, or field), and it was avoided by mariners for the next    350 years.     It was claimed in 1857 by Peter Duncan, an American sea captain, the    third island to be claimed under the Guano Islands Act of 1856 because    of its guano deposits. These deposits were actively mined from 1865 to    1898. Haiti protested the annexation and claimed the island, but the    U.S. rejected the claim and since October 1857 it is claimed by U.S. as    unincorporated territory.     Guano phosphate was a superior organic fertilizer that became a    mainstay of American agriculture in the mid-19th century. Duncan    transferred his discoverer\\'s rights to his employer, an American guano    trader in Jamaica, who sold them to the just-formed Navassa Phosphate    Company of Baltimore. After an interruption for the U.S. Civil War, the    Company built larger mining facilities on Navassa with barrack housing    for 140 black contract laborers from Maryland, houses for white    supervisors, a blacksmith shop, warehouses, and a church. Mining began    in 1865. The workers dug out the guano by dynamite and pick-axe and    hauled it in rail cars to the landing point at Lulu Bay, where it was    sacked and lowered onto boats for transfer to the Company barque, the    S.S. Romance. The living quarters at Lulu Bay were called Lulu Town, as    appears on old maps. Railway tracks eventually extended inland.    Navassa Island.    Enlarge    Navassa Island.     Hauling guano by muscle-power in the fierce tropical heat, combined    with general disgruntlement with conditions on the island eventually    provoked a rebellion on the island in 1889. Five supervisors died in    the fighting. A U.S. warship returned eighteen of the workers to    Baltimore for three separate trials on murder charges. A black    fraternal society, the Order of Galilean Fisherman, raised money to    defend the miners in federal court, and the defense rested its case on    the contention that the men acted in self-defense or in the heat of    passion and that in any case the United States did not have proper    jurisdiction over the island. The cases, including Jones v. United    States, 137 U.S. 202 (1890) went to the U.S. Supreme Court in October    1890, which ruled the Guano Act constitutional, and three of the miners    were scheduled for execution in the spring of 1891. A grass-roots    petition drive by black churches around the country, also signed by    white jurors from the three trials, reached President Benjamin    Harrison, however, who commuted the sentences to imprisonment.     Guano mining resumed on Navassa but at a much reduced level. The    Spanish-American War of 1898 forced the Phosphate Company to evacuate    the island and file for bankruptcy, and the new owners abandoned the    place to the booby birds after 1901.    Navassa Island Light. The light keeper\\'s quarters appear in the    background.    Enlarge    Navassa Island Light. The light keeper\\'s quarters appear in the    background.     Navassa became significant again with the opening of the Panama Canal    in 1914. Shipping between the American eastern seaboard and the Canal    goes through the Windward Passage between Cuba and Haiti. Navassa,    which had always been a hazard to navigation, needed a lighthouse. The    U.S. Lighthouse Service built Navassa Island Light, a 162 foot (46 m)    tower on the island in 1917, 395 feet (120 m) above sea level. A keeper    and two assistants were assigned to live there until the U.S.    Lighthouse Service installed an automatic beacon in 1929. After    absorbing the Lighthouse Service in 1939, the U.S. Coast Guard serviced    the light twice each year. The U.S. Navy set up an observation post for    the duration of World War II. The island has not been inhabited since    then.     A scientific expedition from Harvard University studied the land and    marine life of the island in 1930. Since World War II, amateur radio    operators have landed frequently to broadcast from the territory, which    is accorded \"country\" status by the American Radio Relay League.    Fishermen, mainly from Haiti, fish the waters around Navassa.     From 1903 to 1917 it was a dependency of U.S. Guantanamo Bay Naval    Base, and from 1917 to 1996 it was under U.S. Coast Guard    administration. Since 16 January 1996, it has been administered by U.S.    Department of Interior. On August 29, 1996, the U.S. Coast Guard    dismantled the light on Navassa. An inter-agency task force headed by    the U.S. Department of State transferred the island to the U.S.    Department of the Interior. By Secretary\\'s Order No. 3205 of January    16, 1997, the Interior Department assumed control of the island and    placed the island under its Office of Insular Affairs. A 1998    scientific expedition led by the Centre for Marine Conservation in    Washington D.C. described Navassa as \"a unique preserve of Caribbean    biodiversity.\" The island\\'s land and offshore ecosystems have survived    the twentieth century virtually untouched. The island will be studied    by annual scientific expeditions for the next decade at least.    Aerial photo showing the steep rocky coast that rings the island.    Enlarge    Aerial photo showing the steep rocky coast that rings the island.     By Secretary\\'s Order No. 3210 of December 3, 1999, the U.S. Fish and    Wildlife Service assumed administrative responsibility for Navassa,    which became a National Wildlife Refuge Overlay, also known as Navassa    Island National Wildlife Refuge. The Office of Insular Affairs retains    authority for the island\\'s political affairs and judicial authority is    exercised directly by the nearest U.S. Circuit Court. Access to Navassa    is hazardous and visitors need permission from the Fish and Wildlife    Office in Boqueron, Puerto Rico in order to enter its territorial    waters or land. Since this change of status amateur radio operators    have repeatedly been denied entry. Since, under the callsign KP1, this    is a particularly rare \"entity,\" attempts are being made to have    Congress allow entry. It is understood that, should permission be    received, the island\\'s ecology would be carefully respected.  On the Haiti-US Dispute       * Fabio Spadi (2001) \"Navassa: Legal Nightmares in a Biological        Heaven?\" Boundary & Security Bulletin, autumn edition       * Henry Jones, Plff. in Err., v. United States     Countries in the Caribbean     Independent nations    Commonwealth Realms: Antigua and Barbuda • Bahamas • Barbados • Grenada    • Jamaica • Saint Kitts and Nevis • Saint Lucia • Saint Vincent and the    Grenadines    Commonwealth republics: Dominica • Trinidad and Tobago    Other republics: Cuba • Dominican Republic & Haiti (both on Hispaniola)      __________________________________________________________________     Dependencies    British: Anguilla ∙ British Virgin Islands ∙ Cayman Islands ∙    Montserrat ∙ Turks and Caicos Islands • Dutch: Aruba & Netherlands    Antilles •    French: Guadeloupe & Martinique • U.S.: Navassa Island ∙ Puerto Rico ∙    U.S. Virgin Islands                    Political divisions of the United States                         Capital District of Columbia    States Alabama | Alaska | Arizona | Arkansas | California | Colorado |         Connecticut | Delaware | Florida | Georgia | Hawaii | Idaho |      Illinois | Indiana | Iowa | Kansas | Kentucky | Louisiana | Maine |        Maryland | Massachusetts | Michigan | Minnesota | Mississippi |     Missouri | Montana | Nebraska | Nevada | New Hampshire | New Jersey |        New Mexico | New York | North Carolina | North Dakota | Ohio |      Oklahoma | Oregon | Pennsylvania | Rhode Island | South Carolina |        South Dakota | Tennessee | Texas | Utah | Vermont | Virginia |               Washington | West Virginia | Wisconsin | Wyoming       Insular areas American Samoa | Guam | Northern Mariana Islands |                         Puerto Rico | Virgin Islands    Minor outlying islands Baker Island | Howland Island | Jarvis Island |        Johnston Atoll | Kingman Reef | Midway Atoll | Navassa Island |                          Palmyra Atoll | Wake Island     Retrieved from \" http://en.wikipedia.org/wiki/Navassa_Island\"    This reference article is mainly selected from the English Wikipedia    with only minor checks and changes (see www.wikipedia.org for details    of authors and sources) and is available under the GNU Free    Documentation License. See also our Disclaimer. '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Measure the complexity of an article \n",
    "\n",
    "\n",
    "score = textstat.flesch_reading_ease(project_data['text_content'][1])\n",
    "print(f\"The Flesch Reading-Ease score for the text is: {score}\")\n",
    "\n",
    "gunning_fog_score = textstat.gunning_fog(project_data['text_content'][1])\n",
    "print(f\"The Gunning Fog Index for the text is: {gunning_fog_score}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         title  \\\n",
      "0               Second Crusade   \n",
      "1               Navassa Island   \n",
      "2             Evan Rachel Wood   \n",
      "3  Tropical Storm Henri (2003)   \n",
      "4      Final Fantasy Adventure   \n",
      "\n",
      "                                        text_content  flesch_reading_ease  \\\n",
      "0     #copyright   2007 Schools Wikipedia Selecti...                63.83   \n",
      "1     #copyright   2007 Schools Wikipedia Selecti...                54.73   \n",
      "2     #copyright   2007 Schools Wikipedia Selecti...                52.43   \n",
      "3     #copyright   2007 Schools Wikipedia Selecti...                55.24   \n",
      "4     #copyright   2007 Schools Wikipedia Selecti...                58.92   \n",
      "\n",
      "   gunning_fog_index  \n",
      "0              11.82  \n",
      "1              10.40  \n",
      "2              13.67  \n",
      "3               9.67  \n",
      "4              11.33  \n"
     ]
    }
   ],
   "source": [
    "project_data['flesch_reading_ease'] = project_data['text_content'].apply(textstat.flesch_reading_ease)\n",
    "project_data['gunning_fog_index'] = project_data['text_content'].apply(textstat.gunning_fog)\n",
    "\n",
    "print(project_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Canadian provinces and territories by area 5.09\n",
      "List of sovereign states -81.73\n",
      "List of sovereign states 59.84\n",
      "Babe Ruth 83.25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "min_gunning_fog_index = project_data['gunning_fog_index'].min()\n",
    "title_with_min_gunning_fog_index = project_data.loc[project_data['gunning_fog_index'] == min_gunning_fog_index, 'title'].iloc[0]\n",
    "print(title_with_min_gunning_fog_index, min_gunning_fog_index)\n",
    "\n",
    "min_flesch_reading_ease_index = project_data['flesch_reading_ease'].min() \n",
    "title_with_min_flesch_reading_ease_index = project_data.loc[project_data['flesch_reading_ease'] == min_flesch_reading_ease_index, 'title'].iloc[0]\n",
    "print(title_with_min_flesch_reading_ease_index, min_flesch_reading_ease_index)\n",
    "\n",
    "max_gunning_fog_index = project_data['gunning_fog_index'].max()\n",
    "title_with_max_gunning_fog_index = project_data.loc[project_data['gunning_fog_index'] == max_gunning_fog_index, 'title'].iloc[0]\n",
    "print(title_with_max_gunning_fog_index, max_gunning_fog_index)\n",
    "\n",
    "max_flesch_reading_ease_index = project_data['flesch_reading_ease'].max() \n",
    "title_with_max_flesch_reading_ease_index = project_data.loc[project_data['flesch_reading_ease'] == max_flesch_reading_ease_index, 'title'].iloc[0]\n",
    "print(title_with_max_flesch_reading_ease_index, max_flesch_reading_ease_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
